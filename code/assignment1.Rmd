---
title: "MAST30034 Assignment 1"
author: "Bui Binh An Pham"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1: Synthetic dataset generation, data preprocessing, and data visualization

### 1.1

```{r}
# import required library
library(plot.matrix)
library(abind)
library(ggplot2)
library(reshape2)
```

```{r}
AV = c(0,20,0,0,0,0)
VI = c(30,45,60,40,40,40)
Duration = c(15,20,25,15,20,25)
# create empty matrix to hold the data
TC = matrix(nrow=240)
# generate one temporal source given Arrival, Increment, and Duration
generate_TC <- function(Arrival, Increment, Duration) {
  x = rep(0,240)
  i = Arrival + 1
  while (i <= 240) {
    j = i + Duration - 1
    x[i:j] = 1
    i = i + Increment
  } 
  return(x)
}
# construct a matrix TC of six temporal sources
for (i in 1:length(AV)) {
  single_TC = generate_TC(AV[i], VI[i], Duration[i])
  TC = cbind(TC, single_TC)
}
TC = TC[,-1]
colnames(TC) = NULL
# standardize each temporal source to have mean 0 and std 1
TC_scaled = scale(TC)
# save TC to file
write.csv(TC_scaled, file="../data/TC.csv", row.names = FALSE)

# plot all TCs as six subplots
par(mfrow=c(2,3), mar=c(2,3,3,3))
for (i in 1:ncol(TC)) {
  data = TC_scaled[,i]
  plot(data, type='l', xlab='', ylab='', main=paste("TC ", i))
}
```

With normalization, TCs become biased (they will have the same range but not the same mean) and are not equally important (their standard deviation are not equal). Therefore, standardization is preferred as it can make TCs bias-free and equally important.

### 1.2

```{r}
# generate correlation matrix
CM = cor(TC)
par(mfrow=c(1,1))
plot(CM, border=NA, ylab='', xlab='', main="CM of TCs")
```

We can observe that TC 4 & 5 and TC 5 & 6 are highly correlated.

### 1.3

```{r}
tmpSM = c()
vertical = c(c(2,6), c(2,6), c(8,13), c(8,13), c(15,19), c(15,19))
horizontal = c(c(2,6), c(15,19), c(2,6), c(15,19), c(2,6), c(15,19))
i = 1
while (i <= 12) {
  tmp = matrix(nrow=21, ncol=21, 0)
  tmp[vertical[i]:vertical[i+1],horizontal[i]:horizontal[i+1]] = 1
  tmpSM = abind(tmpSM, tmp, along=3)
  i = i + 2
}

# plot these SMs in six subplot
par(mfrow=c(2,3), mar=c(2,4,2,4))
for (i in 1:6) {
  data = tmpSM[,,i]
  plot(data, border=NA, xlab='', ylab='', main=paste("SM ", i))
}
```

```{r}
# reshape array into 2D matrix with shape 6*441
SM = t(matrix(tmpSM, 21*21, 6))
# save to file
write.csv(SM, file='../data/SM.csv', row.names = FALSE)

# correlation between SMs
par(mfrow=c(1,1))
plot(cor(t(SM)), border=NA, ylab='', xlab='', main="CM of SMs")
```

From the correlation plot, it is clear that these six vectored SMs are independent. As SM records the spatial information, it contains the location information of our target object. For example, in an image layer, pixels that have values of one, while the rest are zeros, signal the area where the object exists. If all the pixel values or only one of them change from 1 to 5, we can still discern the target object's location since the boundary of the target object does not change. Therefore, standardization of SM in our case is not as crucial as TC, where the change can distort the trend of time-series data.

```{r, echo=FALSE}
par(mfrow=c(1,2), mar=c(2,3,4,3))
TC_tmp = TC[,1]
TC_tmp[36] = 5
SM_tmp = tmpSM[,,1]
SM_tmp[4,4] = 5
plot(TC_tmp, type='l', main='Unstandardized TC 1 \n with one pixel \n value of 5', cex.main=0.8)
plot(SM_tmp, border=NA, main='Unstandardized SM 1 \n with one pixel \n value of 5', cex.main=0.8)
```

### 1.4

```{r}
# generate white Gaussian noise
set.seed(156)
temporal_noise = rnorm(240*6, mean=0, sd=sqrt(0.25))
temporal_noise = matrix(temporal_noise, nrow=240, ncol=6)

spatial_noise = rnorm(6*441, mean=0, sd=sqrt(0.015))
spatial_noise = matrix(spatial_noise, nrow=6, ncol=441)

# correlation matrix for each noise type
par(mfrow=c(1,2))
plot(cor(temporal_noise), border=NA, xlab="", ylab="", main="Temporal noise")
plot(cor(t(spatial_noise)), border=NA, xlab="", ylab="", main="Spatial noise")

#histogram of both noise sources
par(mfrow=c(1,2), mar=c(2,2,2,2))
hist(as.vector(temporal_noise), main="Temporal noise", freq=FALSE)
lines(density(as.vector(temporal_noise)),col='red')
hist(as.vector(spatial_noise), main="Spatial noise", freq=FALSE)
lines(density(as.vector(spatial_noise)),col='red')
```

We observe some small correlations across different sources for each noise type, spatial and temporal, from the correlation plot. For example, there is a negative correlation between sources 5 & 6 in temporal noise and 4 & 5 in spatial noise. Moreover, we can confirm from the histogram that both temporal and spatial noise follows a normal distribution with mean zero and variance 0.25 and 0.015, respectively. The Shapiro test for normality also agrees with this observation.

```{r}
# p-value = 0.3973 > 0.05, can not reject the null hypothesis, 
# implying temporal noise follow normal distribution
shapiro.test(as.vector(temporal_noise))
# p-value = 0.9198 > 0.05, can not reject the null hypothesis, 
# implying spatial noise also follow normal distribution
shapiro.test(as.vector(spatial_noise))
```

```{r}
# product of temporal and spatial source
temporal_spatial_noise = temporal_noise %*% spatial_noise
par(mfrow=c(1,1), mar=c(3,3,4,3))
plot(cor(temporal_spatial_noise), border=NA,
     main="Correlation plot of product \n of temporal and spatial noise \n across all variables",
     xlab="", ylab="", cex.main=0.8)

# correlation plot on small subset of variable 
# so it is easier to observe whether there 
# exists a correlation of the product of temporal
# and spatial noises across variables.
temporal_spatial_noise = temporal_spatial_noise[,1:10]
plot(cor(temporal_spatial_noise), border=NA,
     main="Correlation plot of product \n of temporal and spatial noise \n across first 10 variables",
     xlab="", ylab="", cex.main=0.8)
```

By plotting only a subset of variables, we observe a correlation of the product of temporal and spatial noises between different variables. For example, considering the first ten variables, the product of temporal and spatial noise of the 9th variable correlates with the 8th variable, the 9th with the second variable, the 8th with the second variable.

### 1.5

```{r}
# generate a synthetic dataset
X = (TC + temporal_noise) %*% (SM + spatial_noise)

# plot 100 randomly selected time-series from X
set.seed(156)
X_df = data.frame(X)
sample_col_idx = sample(ncol(X_df), 100)
X_df = X_df[,sample_col_idx]
X_df['index']=1:240

# convert data into long format to use ggplot2
X_df_long = melt(X_df, id.vars = c('index'))

par(mfrow=c(1,1), mar=c(2,2,2,2))
p = ggplot(X_df_long, aes(x=index, y=value, group=variable, color=variable)) + geom_line() 
p = p + ggtitle("100 Randomly selected time-series")
p = p + theme(legend.position = "none")
p = p + labs(x="")
print(p)


```

```{r}
# print legend seperately
par(mfrow=c(1,1), mar=c(2,2,2,2))
p = ggplot(X_df_long, aes(x=index, y=value, group=variable, color=variable)) + geom_line() 
p = p + ggtitle("100 Randomly selected time-series")
p = p + theme(legend.position = "bottom")
p = p + guides(color=guide_legend(ncol=8))
p = p + labs(x="")

library(cowplot)
library(grid)
legend = get_legend(p)
grid.newpage()
grid.draw(legend)
```

```{r}
# plot variance of all 441 variable
par(mfrow=c(1,1), mar=c(4,4,2,2))
plot(diag(var(X)), main='Variance of all 441 variables', xlab="Variable", ylab="Variance")

# standardized dataset
X = scale(X)

# save the dataset
write.csv(X, file="../data/data.csv", row.names=FALSE)
```

The variance plot shows that the variance increases from the 300th variable (time-series) to 400th and 100th to the first. The variance is not constant throughout the time series, which violates the assumption for linear regression. Before fitting linear models, we need to transform the data.

## Question 2: Data analyst, results visualization, and performance metrics

### 2.1

```{r}
# import required library
library(plot.matrix)
library(abind)
```

```{r}
# import dataset
X = as.matrix(read.csv("../data/data.csv"))
TC = as.matrix(read.csv("../data/TC.csv"))
SM = as.matrix(read.csv("../data/SM.csv"))

# estimate A(retrieval of SM) and D(TC) using least square solution
A_LSR = solve(t(TC)%*%TC)%*%t(TC)%*%X
D_LSR = X%*%t(A_LSR)

# plot six retrieved spatial and temporal sources
par(mfrow=c(3,2), mar=c(2,3,2,3))
for (i in 1:6) {
  # apply abs on retrieved coefficient values A, making it easer to visualize
  spatial = abs(A_LSR[i,])
  spatial = matrix(spatial, 21, 21)
  plot(spatial, main=paste("Retrieved SM ", i), border=NA, ylab='', mar=c(2,4,2,4))
  temporal = D_LSR[,i]
  plot(temporal, type='l', ylab='', xlab='', main=paste('Retrieved TM ', i), cex=0.5)
}

par(mfrow=c(2,1), mar=c(4,4,2,2))
# scatter plot between 3rd column of D and 30th column of X
plot(x=D_LSR[,3], y=X[,30], xlab='3rd column of D', ylab='30th column of X')
# scatter plot between 3rd column of D and 30th column of X
plot(x=D_LSR[,4], y=X[,30], xlab='4th column of D', ylab='30th column of X')
```

From the arrangement of the slices, we observe `SM[3,30] = 1` while `SM[4,30] = 0`, implying that only the third column of D (TC 3) contribute to the creation of the 30th data element in X. Since the coefficient equals one, it also explains the perfect linear relationship between 3rd column of D and 30th column of X.

### 2.2

```{r}
# ridge regression
lambda = 0.7 * 441
A_RR = solve(t(TC)%*%TC + lambda*diag(6))%*%t(TC)%*%X
D_RR = X%*%t(A_RR)

# correlation vectors retaining only maximum absolute
c_TLSR = abs(diag(cor(TC, D_LSR)))
c_TRR = abs(diag(cor(TC, D_RR)))
# verify that sum(c_TRR)>sum(c_TLSR) with our current 
# choice of lambda=0.7
sum(c_TRR)>sum(c_TLSR)

# ridge regression with lambda=1000
lambda = 1000 * 441
A_RR_1000 = solve(t(TC)%*%TC + lambda*diag(6))%*%t(TC)%*%X
cbind("a_RR" = A_RR_1000[,1], "a_LSR" = A_LSR[,1])
```

We see that all values in the first vector of `A_RR` are being shrunk towards zero, compared with `A_LSR`.

### 2.3

```{r}
# Code for LR given in assignment specification
lasso_regression <- function(TC, rho, N, nsrcs, x1, x2) {
  step = 1/(norm(TC %*% t(TC)) * 1.1)
  thr = rho*N*step
  Ao = matrix(0, nsrcs, 1)
  A = matrix(0, nsrcs, 1)
  Alr = matrix(0, nsrcs, x1*x2)
  
  for (k in 1:(x1*x2)) {
    A = Ao+step*(t(TC) %*% (X[,k]-(TC%*%Ao)))
    A = (1/(1+thr)) * (sign(A)*pmax(replicate(nsrcs, 0), abs(A)-thr))
    
    for (i in 1:10) {
      Ao = A
      A = Ao+step * (t(TC)%*%(X[,k]-(TC%*%Ao)))
      A = (1/(1+thr)) * (sign(A)*pmax(replicate(nsrcs, 0), abs(A)-thr))
    }
    Alr[,k] = A
  }
  return(Alr)
}

# calculate MSEs for lasso regression with range of different
# rho values: 21 values between 0 and 1 with interval of 0.05.
calculate_MSEs <- function(X, TC) {
  rhos = seq(0,1,0.05)
  N = 240; V=441; nsrcs=6; x1=21; x2=21
  MSEs = c()
  for (i in 1:length(rhos)) {
    A_LR = lasso_regression(TC, rhos[i], N=N, nsrcs=6, x1=x1, x2=x2)
    D_LR = X%*%t(A_LR)
    MSE = sum(sum((X-D_LR%*%A_LR)^2))/(N*V)
    MSEs = c(MSEs, MSE)
  } 
  return(MSEs)
}

# generate new standardized dataset X 
generate_X <- function(TC, SM, seed) {
  set.seed(seed)
  # generate white Gaussian noise
  temporal_noise = rnorm(240*6, mean=0, sd=sqrt(0.25))
  temporal_noise = matrix(temporal_noise, nrow=240, ncol=6)
  
  spatial_noise = rnorm(6*441, mean=0, sd=sqrt(0.015))
  spatial_noise = matrix(spatial_noise, nrow=6, ncol=441)
  
  X = (TC + temporal_noise) %*% (SM + spatial_noise)
  # standardized dataset
  X = scale(X)
  return(X)
}

# seed=156 is the seed used in question 1 to generate X
SEED = c(156,157,158,159,160,161,162,163,164,165)
MSE_all = c()
# perform 10 realizations
for (i in 1:10) {
  X = generate_X(TC, SM, SEED[i])
  MSEs = calculate_MSEs(X, TC)
  MSE_all = abind(MSE_all, MSEs, along=2)
}

# average of MSE over 10 realizations
MSE_avg = rowMeans(MSE_all)
par(mfrow=c(1,1))
plot(x=seq(0,1,0.05), y=MSE_avg, type="l", col="red", xlab=expression(paste("",rho)))
# at rho=0.65, MSE value start to increase again. 
# at rho=0.6, MSE value is at its minimum
```

MSE reaches its minimum at 0.628 when `rho=0.25`, and from `rho=0.3` MSE value starts to increase again. We decide to choose `rho=0.25` as our final regularization term for the LASSO regression.

### 2.4

```{r}
# using generated data from question 1
X = as.matrix(read.csv("../data/data.csv"))
# estimate LR parameters for rho=0.25
A_LR = lasso_regression(TC, 0.25, N=240, nsrcs=6, x1=21, x2=21)
D_LR = X%*%t(A_LR)

# correlation vectors retaining only maximum absolute value
c_TRR = abs(diag(cor(TC, D_RR)))
c_SRR = abs(diag(cor(t(SM), t(A_RR))))
c_TLR = abs(diag(cor(TC, D_LR)))
c_SLR = abs(diag(cor(t(SM), t(A_LR))))
# verify that our choice of regularization term rho is correct
sum(c_TLR)>sum(c_TRR)
sum(c_SLR)>sum(c_SRR)
```

```{r}
# plot D and A side by side for both Ridge Regression and Lasso Regression
par(mfrow=c(3,4), mar=c(2,3,3,3))
for (i in 1:6) {
  plot(abs(matrix(A_RR[i,],21,21)), border=NA, main=paste("A_RR ", i))
  plot(D_RR[,i], type='l', main=paste("D_RR ", i))
  plot(abs(matrix(A_LR[i,],21,21)), border=NA, main=paste("A_LR ", i))
  plot(D_LR[,i], type='l', main=paste("D_LR ", i))
}
```

In ridge regression, all coefficients are shrunk towards zero, but none of them equals zero. Hence ridge regression is not able to eliminate features that have low predictive power. In contrast, lasso regression is more robust as it can eliminate less important attributes by making their coefficients equal to zero. Therefore, when compared with lasso regression, ridge regression produces many false positives as it still incorporates noisy, undesired features.

### 2.5

```{r}
# principal component regression
s = svd(TC)
PC = s$u
eigenvalues = s$d

# plot eigenvalues
par(mfrow=c(1,1), mar=c(4,4,3,3))
plot(eigenvalues, type='l', xlab="PC", ylab='Eigenvalue') 

```

The eigenvalue of the 6 PC is the lowest at 2.995.

```{r}
# plot the regressors in Z and source TC
par(mfrow=c(3,2), mar=c(3,3,3,3))
for (i in 1:6) {
  plot(TC[,i], type='l', main=paste("TC ", i))
  plot(PC[,i], type='l', main=paste("PC ", i))
}
```

We notice that there is a shape deterioration when moving from TCs to PCs. This loss of shape can be attributed to the nature of PC; each PC is a linear combination of all TCs that capture the most significant amount of variance. Hence, for example, the shape of PC 1 is not similar to TC 1 since it is a combination of all TCs, representing the trend across TC sources. Similarly, PC 2 is also a combination of all TCs, but it captures the movement across TCs that has not been accounted for in PC 1. All PCs behave in the same manner, explaining their deteriorated shape when comparing with TCs.

```{r}
# estimate PCR parameters for rho=0.001
A_PCR = lasso_regression(PC, 0.001, N=240, nsrcs=6, x1=21, x2=21)
D_PCR = X%*%t(A_PCR)

# plot D_PCR and A_PCR side by side
par(mfrow=c(3,2), mar=c(2,3,3,3))
for (i in 1:6) {
  plot(abs(matrix(A_PCR[i,],21,21)), border=NA, main=paste("A_PCR ", i))
  plot(D_PCR[,i], type='l', main=paste("D_PCR ", i))
}
```

```{r}
# compare PCR with LSR, RR, LR in retrieving the 
# first SM
par(mfrow=c(2,2), mar=c(3,3,3,3))
plot(abs(matrix(A_LSR[1,],21,21)), border=NA, main=paste("A_LSR ", 1))
plot(abs(matrix(A_RR[1,],21,21)), border=NA, main=paste("A_RR ", 1))
plot(abs(matrix(A_LR[1,],21,21)), border=NA, main=paste("A_LR ", 1))
plot(abs(matrix(A_PCR[1,],21,21)), border=NA, main=paste("A_PCR ", 1))
```

The figure shows that lasso regression is the most robust and produces the smallest number of false positives pixels (colored yellow instead of red outside the slice `[2:6,2:6]`). It is followed by ridge regression and least square regression with substantially more incorrect pixels. However, principal component regression performs the worst as the original slice `[2:6,2:6]` could not be retrieved correctly (supposed to be colored yellow). The poor performance of PCR can be attributed to the fact that the weight given to underlying original features only partly depends on their correlation with the target variable. The weight on the original features also depends on the eigenvector as each is a linear combination of the original features, which has little relation to the target predictor. Since PCR uses principal components of original variables as regressors, the underlying noise variable may be given a larger weight than the explanatory variable due to having a significant eigenvector weight component. As the PCR can only control the weight given to its regressors (PCs) but not the weight given to the original features (TCs) by principal components, the model fails to eliminate noisy attributes and focus on high predictive features.
